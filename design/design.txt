With the emergence of Siri and Cortana text to speech (as well as speech to text) programs have grown rapidly. However most of these programs only have one voice and focus instead on the applications of a personal assistance. I plan on instead working on a method to easily extract the different sounds an individual makes and manipulating that sound. There are also other python based text to speech bots that are more manipulatable. Although only being able to produce one voice you are able to change the speed that this voice speaks, pitch, amplitude etc. which, if given enough time, would be an interesting addition to my project for those people who wish to hear what their voice sounds like when manipulated. However, again many of these online python programs only offer selective voices and lack the customisability that my program will offer. Some similarities between my program and these online examples are in the uses. Many of these programs are used to read out loud articles, notes or whatever the user wants and by copy and pasting the words or possibly by selecting the file you can also manage to do read any selection aloud in your own voice or in the voice of a friend. For my term project I created a personalised text to speech program. The user has the option to either create their own text to speech or use an existing one. To solve the problem of creating a new text to speech I use audio spectrograph analysis to find the formants or high energy zones that are like fingerprints for the phonetic sounds. Based off of the scipy packages and existing online code that I edited to fit my purposes I was able to find the formants for a section of audio. So when a user reads out a word using this spectrograph analysis I run through the frames of the word to find what section has a similar fingerprint to the already existing phonetic sound I am search for. I do this for all 44 phonetic sounds in the english language and save the section of audio frames to a dictionary. Once I have all of the audio sounds together I save them into a text file for use in the text to speech. To accomplish this second part of my project I first gather the phonetic sounds from the words the user wants to hear through the dictionary.com html code using the requests package. Then I combine the appropriate sounds from the saved dictionary. However because by just adding the sounds one after another makes the words sounds chopy so instead the program uses the audioop.add function to combine a certain variable percentage of the sounds together to make it sounds closer to human. Some other problems that I solved are if the user requests words not in the dictionary so the user also has the option to add specific words. My design for the user interface is simply based off of functionality. When the user is creating the new text to speech program I allow the user to be able to make the sound and test the audio sparsing by hearing the phonetic section by ear. For the rest of this program I follow a similar approach allowing the user to be able to control almost every part of the process in the simplest way possible. Overall this project lead to some interesting problems with audio analysis, which to solve lead to me learning a lot about phonetic sounds and audio spectrograph and some interesting results. For my project audio sparsing did better than I expected, being able to find audio sounds from any area in word and the user interface which ended up cleaner than I expected.    